@report{AmtfurStadterneuerungundBodenmanagement-StadtEssen2020,
address = {Essen},
author = {{Amt f{\"{u}}r Stadterneuerung und Bodenmanagement - Stadt Essen}},
file = {:C\:/Users/daher/Downloads/Mietspiegelbroschuere_2020_DE.pdf:pdf},
institution = {Amt f{\"{u}}r Stadterneuerung und Bodenmanagement},
keywords = {Mietspiegel},
mendeley-groups = {Master Thesis,Master Thesis/_Wohnungmarkt},
mendeley-tags = {Mietspiegel},
pages = {19},
title = {{Mietspiegel 2020 - f{\"{u}}r nicht preisgebundene Wohnungen in Essen}},
year = {2020}
}
@article{Asaftei2018,
address = {Washington, DC},
author = {Asaftei, Gabriel Morgan and Doshi, Sudeep and Means, John and Sanghvi, Aditya},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asaftei et al. - 2018 - Getting ahead of the market How big data is transforming real estate.pdf:pdf},
institution = {Urban Land Institute},
journal = {Urban Land},
mendeley-groups = {Master Thesis,Master Thesis/_Wohnungmarkt},
pages = {6},
title = {{Getting ahead of the market: How big data is transforming real estate}},
year = {2018}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
mendeley-groups = {Master Thesis/RandomForest},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Cerda2018,
abstract = {For statistical learning, categorical variables in a table are usually considered as discrete entities and encoded separately to feature vectors, e.g., with one-hot encoding. “Dirty” non-curated data give rise to categorical variables with a very high cardinality but redundancy: several categories reflect the same entity. In databases, this issue is typically solved with a deduplication step. We show that a simple approach that exposes the redundancy to the learning algorithm brings significant gains. We study a generalization of one-hot encoding, similarity encoding, that builds feature vectors from similarities across categories. We perform a thorough empirical validation on non-curated tables, a problem seldom studied in machine learning. Results on seven real-world datasets show that similarity encoding brings significant gains in predictive performance in comparison with known encoding methods for categories or strings, notably one-hot encoding and bag of character n-grams. We draw practical recommendations for encoding dirty categories: 3-gram similarity appears to be a good choice to capture morphological resemblance. For very high-cardinalities, dimensionality reduction significantly reduces the computational cost with little loss in performance: random projections or choosing a subset of prototype categories still outperform classic encoding approaches.},
author = {Cerda, Patricio and Varoquaux, Ga{\"{e}}l and K{\'{e}}gl, Bal{\'{a}}zs},
doi = {10.1007/s10994-018-5724-2},
eprint = {1806.00979},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cerda, Varoquaux, K{\'{e}}gl - 2018 - Similarity encoding for learning with dirty categorical variables.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Categorical variables,Dirty data,Statistical learning,String similarity measures},
mendeley-groups = {Master Thesis/Aufbereitung},
month = {sep},
number = {8-10},
pages = {1477--1494},
publisher = {Springer New York LLC},
title = {{Similarity encoding for learning with dirty categorical variables}},
volume = {107},
year = {2018}
}
@inproceedings{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
author = {Chen, Tianqi and Guestrin, Carlos},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Guestrin - 2016 - XGBoost A scalable tree boosting system.pdf:pdf},
isbn = {9781450342322},
keywords = {Large-scale Machine learning},
mendeley-groups = {Master Thesis/XGBoost},
pages = {785--794},
shorttitle = {Xgboost},
title = {{XGBoost: A scalable tree boosting system}},
volume = {13-17-Augu},
year = {2016}
}
@book{Goodfellow2017,
abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
file = {:C\:/IT/Literatur/Goodfellow - DeepLearning.pdf:pdf},
isbn = {0262035618},
keywords = {Deep Learning,Machine Learnings},
mendeley-groups = {CNN / Computer Vision,Master Thesis,Master Thesis/MLP},
mendeley-tags = {Deep Learning,Machine Learnings},
pages = {800},
publisher = {The MIT Press},
title = {{Deep Learning (Adaptive Computation and Machine Learning)}},
year = {2017}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
file = {:C\:/Users/daher/Downloads/hornik1989.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
mendeley-groups = {Master Thesis/MLP},
number = {5},
pages = {359--366},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@book{Larose2015,
author = {Larose, Daniel T. and Larose, Chantal D.},
edition = {Second Edition},
file = {:C\:/IT/Literatur/Larose, - Data Mining and Predictive Analytics-Wiley (2015).pdf:pdf},
isbn = {978-1-118-11619-7},
mendeley-groups = {Master Thesis},
pages = {794},
publisher = {Wiley},
title = {{Data mining and predictive analytics}},
year = {2015}
}
@article{Wirth2000,
abstract = {The CRISP-DM (CRoss Industry Standard Process for Data Mining) project proposed a comprehensive process model for carrying out data mining projects. The process model is independent of both the industry sector and the technology used. In this paper we argue in favor of a standard process model for data mining and report some experiences with the CRISP-DM process model in practice. We applied and tested the CRISP-DM methodology in a response modeling application project. The final goal of the project was to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations. The initial projects were performed by experienced data mining people; future projects are to be performed by people with lower technical skills and with very little time to experiment with different approaches. It turned out, that the CRISP-DM methodology with its distinction of generic and specialized process models provides both the structure and the flexibility necessary to suit the needs of both groups. The generic CRISP-DM process model is useful for planning, communication within and outside the project team, and documentation. The generic check-lists are helpful even for experienced people. The generic process model provides an excellent foundation for developing a specialized process model which prescribes the steps to be taken in detail and which gives practical advice for all these steps.},
author = {Wirth, R{\"{u}}diger},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wirth, Hipp - Unknown - CRISP-DM Towards a Standard Process Model for Data Mining.pdf:pdf},
journal = {Proceedings of the Fourth International Conference on the Practical Application of Knowledge Discovery and Data Mining},
mendeley-groups = {Master Thesis,Master Thesis/Struktur},
number = {24959},
pages = {29--39},
title = {{CRISP-DM : Towards a Standard Process Model for Data Mining}},
year = {2000}
}
@article{Rahm2000,
abstract = {Scheduling, logging, , recovery, backup for loading and refreshing a or during 4]. To support , detailed information about},
author = {Rahm, Erhard and Do, Hong Hai},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahm, Do - 2000 - Data cleaning Problems and current approaches.pdf:pdf},
isbn = {9781595938275},
issn = {03064379},
journal = {IEEE Data Eng. Bull.},
keywords = {Cleansing,ETL-Architektur},
mendeley-groups = {Master Thesis/Aufbereitung},
number = {4},
pages = {3--13},
title = {{Data cleaning: Problems and current approaches}},
volume = {23},
year = {2000}
}
@article{Box1964,
abstract = {In the analysis of data it is often assumed that observations y,, y,,...,y, are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters 0. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples. 1.},
author = {Box, G. E. P. and Cox, D. R.},
doi = {10.1111/j.2517-6161.1964.tb00553.x},
file = {:C\:/Users/daher/Downloads/box1964.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
mendeley-groups = {Master Thesis/Aufbereitung},
month = {jul},
number = {2},
pages = {211--243},
publisher = {Wiley},
title = {{An Analysis of Transformations}},
volume = {26},
year = {1964}
}
@report{StadtEssen2018,
address = {Stadt Essen},
author = {{Stadt Essen} and {InWIS Forschung}},
file = {:C\:/Users/daher/Downloads/Wohnungsnachfrageanalyse_Essen_2025_InWIS.pdf:pdf},
institution = {InWIS Forschung \& Beratung},
mendeley-groups = {Master Thesis/_Wohnungmarkt},
pages = {84},
title = {{Wohnungsnachfrageanalyse Essen 2025+}},
year = {2018}
}
@book{Brause1991,
address = {Stuttgard},
author = {Brause, R{\"{u}}diger},
doi = {10.1007/978-3-322-92118-5},
file = {:C\:/Users/daher/Downloads/brause1991.pdf:pdf},
isbn = {978-3-519-02247-3},
mendeley-groups = {Master Thesis/MLP},
pages = {294},
publisher = {Teubner Verlag},
title = {{Neuronale Netze: eine Einf{\"{u}}hrung in die Neuroinformatik}},
year = {1991}
}
@article{Gardner1998,
abstract = {Artificial neural networks are appearing as useful alternatives to traditional statistical modelling techniques in many scientific disciplines. This paper presents a general introduction and discussion of recent applications of the multilayer perceptron, one type of artificial neural network, in the atmospheric sciences.},
author = {Gardner, M. W. and Dorling, S. R.},
doi = {10.1016/S1352-2310(97)00447-0},
file = {:C\:/Users/daher/Downloads/gardner1998.pdf:pdf},
issn = {13522310},
journal = {Atmospheric Environment},
keywords = {Artificial intelligence,Backpropagation,Neural network,Statistical modelling},
mendeley-groups = {Master Thesis,Master Thesis/MLP},
month = {aug},
number = {14-15},
pages = {2627--2636},
publisher = {Elsevier Sci Ltd},
title = {{Artificial neural networks (the multilayer perceptron) - a review of applications in the atmospheric sciences}},
volume = {32},
year = {1998}
}
@article{Anguita2012,
abstract = {The K-fold Cross Validation (KCV) technique is one of the most used approaches by practitioners for model selection and error estimation of classifiers. The KCV consists in splitting a dataset into k subsets; then, iteratively, some of them are used to learn the model, while the others are exploited to assess its performance. However, in spite of the KCV success, only practical rule-of-thumb methods exist to choose the number and the cardinality of the subsets. We propose here an approach , which allows to tune the number of the subsets of the KCV in a data-dependent way, so to obtain a reliable, tight and rigorous estimation of the probability of misclassification of the chosen model.},
address = {Br{\"{u}}gge},
author = {Anguita, Davide and Ghelardoni, Luca and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anguita et al. - Unknown - The 'K' in K-fold Cross Validation.pdf:pdf},
institution = {University of Genova - Department of Biophysical and Electronic Engineering},
isbn = {9782874190490},
mendeley-groups = {Master Thesis/Aufbereitung},
pages = {6},
publisher = {ESANN 2012 proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
title = {{The 'K' in K-fold Cross Validation}},
year = {2012}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
doi = {10.5555/2188385.2188395},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Ca, Ca - 2012 - Random Search for Hyper-Parameter Optimization Yoshua Bengio.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
mendeley-groups = {Master Thesis/Aufbereitung},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@book{Verbeek2017,
address = {Rotterdam School of Management, Erasmus University, Rotterdam},
author = {Verbeek, Marno},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeek - 2017 - A guide to modern econometrics.pdf:pdf},
isbn = {978-1-119-40115-5},
mendeley-groups = {Master Thesis,Master Thesis/LinReg},
pages = {508},
publisher = {John Wiley \& Sons},
title = {{A guide to modern econometrics}},
year = {2017}
}
@report{LEG2019,
address = {D{\"{u}}sseldorf},
author = {LEG and CBRE},
file = {:C\:/Users/daher/Downloads/2019-09-26_LEG_Wohnungsmarktreport_NRW_2019.pdf:pdf},
institution = {LEG Immobilien AG},
mendeley-groups = {Master Thesis,Master Thesis/_Wohnungmarkt},
pages = {33},
title = {{LEG-Wohnungsmarktreport 2019}},
year = {2019}
}
@article{Mayring2001,
abstract = {In this paper, I am going to outline ways of combining qualitative and quantitative steps of analysis on five levels. On the technical level, programs for the computer-aided analysis of qualitative data offer various combinations. Where the data are concerned, the employment of categories (for instance by using qualitative content analysis) allows for combining qualitative and quantitative forms of data analysis. On the individual level, the creation of types and the inductive generalisation of cases allow for proceeding from individual case material to quantitative generalisations. As for research design, different models can be distinguished (preliminary study, generalisation, elaboration, triangulation) which combine qualitative and quantitative steps of analysis. Where the logic of research is concerned, it can be shown that an extended process model which combined qualitative and quantitative research can be appropriate and thus lead to an integration of the two approaches. URN: urn:nbn:de:0114-fqs010162},
author = {Mayring, Philipp},
doi = {10.17169/fqs-2.1.967},
file = {:C\:/Users/daher/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayring - 2001 - Combination and Integration of Qualitative and Quantitative Analysis.pdf:pdf},
issn = {1438-5627},
journal = {Forum: Qualitative Social Research},
keywords = {Methodenintegration,Methodologie,Qualitative Forschung,Qualitative Inhaltsanalyse,Quantitative Forschung,integration of methods,methodology,qualitative content analysis,qualitative research,quantitative research},
mendeley-groups = {Master Thesis,Master Thesis/Struktur},
month = {feb},
number = {1},
title = {{Combination and Integration of Qualitative and Quantitative Analysis}},
volume = {2},
year = {2001}
}

